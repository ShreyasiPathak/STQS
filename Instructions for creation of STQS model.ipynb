{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of Execution of the codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Aligning the edf with the sleep stage annotation and converting .edf files to .mat files.**<br/>\n",
    "Takes as input the path to .edf signal files and the path to .xml sleep annotation files<br/>\n",
    "Outputs the .mat files aligning the signals to sleep stage annotation<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "python data_extraction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Creating 2 separate pkl files containing filename (.mat filename) with its corresponding length (in terms of 30 secs epochs) and containing filenames with its corresponding cumulative length (e.g. length of 1st file, length of 1st+2nd file, length of 1st+2nd+3rd file etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 eeg_annotation_shhs1-200004\n",
      "1 eeg_annotation_shhs1-200007\n",
      "2 eeg_annotation_shhs1-200008\n",
      "3 eeg_annotation_shhs1-200003\n",
      "4 eeg_annotation_shhs1-200002\n",
      "0 eeg_annotation_shhs1-200004\n",
      "1 eeg_annotation_shhs1-200007\n",
      "2 eeg_annotation_shhs1-200008\n",
      "3 eeg_annotation_shhs1-200003\n",
      "4 eeg_annotation_shhs1-200002\n",
      "0 eeg_annotation_shhs1-200006\n",
      "1 eeg_annotation_shhs1-200010\n",
      "2 eeg_annotation_shhs1-200001\n",
      "3 eeg_annotation_shhs1-200005\n",
      "4 eeg_annotation_shhs1-200009\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "path_to_mat_folder='D:/DEEPSLEEP/test_for_github/datasets/shhs/mat files/'\n",
    "path_to_file_length_train='D:/DEEPSLEEP/test_for_github/datasets/shhs/train/trainFilesNum30secEpochs_all_shhs1.pkl'\n",
    "path_to_file_length_train_cumul='D:/DEEPSLEEP/test_for_github/datasets/shhs/train/trainFilesNum30secEpochsCumulative_all_shhs1.pkl'\n",
    "path_to_file_length_val='D:/DEEPSLEEP/test_for_github/datasets/shhs/val/valFilesNum30secEpochs_all_shhs1.pkl'\n",
    "\n",
    "random.seed(30)\n",
    "mat_files=[]\n",
    "\n",
    "for i in os.listdir(path_to_mat_folder):\n",
    "    mat_files.append(i)\n",
    "shuffle(mat_files)\n",
    "training_set=5\n",
    "validation_set=10\n",
    "\n",
    "utils.calculate_num_samples(mat_files[:training_set],path_to_file_length_train)\n",
    "utils.calculate_num_samples_cumulative(mat_files[:training_set],path_to_file_length_train_cumul)\n",
    "utils.calculate_num_samples(mat_files[training_set:validation_set],path_to_file_length_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Creating 3 separate hdf5 files from all the .mat files in the training set, in validation set and in test ste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 eeg_annotation_shhs1-200004\n",
      "(875, 1, 5, 3750)\n",
      "(875, 1)\n",
      "1 eeg_annotation_shhs1-200007\n",
      "(919, 1, 5, 3750)\n",
      "(919, 1)\n",
      "2 eeg_annotation_shhs1-200008\n",
      "(959, 1, 5, 3750)\n",
      "(959, 1)\n",
      "3 eeg_annotation_shhs1-200003\n",
      "(1049, 1, 5, 3750)\n",
      "(1049, 1)\n",
      "4 eeg_annotation_shhs1-200002\n",
      "(1079, 1, 5, 3750)\n",
      "(1079, 1)\n",
      "dset data shape outside: (4881, 1, 5, 3750)\n",
      "dset label shape outside: (4881, 1)\n",
      "0 eeg_annotation_shhs1-200006\n",
      "(1084, 1, 5, 3750)\n",
      "(1084, 1)\n",
      "1 eeg_annotation_shhs1-200010\n",
      "(1084, 1, 5, 3750)\n",
      "(1084, 1)\n",
      "2 eeg_annotation_shhs1-200001\n",
      "(1084, 1, 5, 3750)\n",
      "(1084, 1)\n",
      "3 eeg_annotation_shhs1-200005\n",
      "(1084, 1, 5, 3750)\n",
      "(1084, 1)\n",
      "4 eeg_annotation_shhs1-200009\n",
      "(1086, 1, 5, 3750)\n",
      "(1086, 1)\n",
      "dset data shape outside: (5422, 1, 5, 3750)\n",
      "dset label shape outside: (5422, 1)\n"
     ]
    }
   ],
   "source": [
    "path_to_hdf5_file_train='D:/DEEPSLEEP/test_for_github/datasets/shhs/train/hdf5_file_train_all_chunking_shhs1.hdf5'\n",
    "utils.hdf5_creation1(mat_files[:training_set],path_to_hdf5_file_train)\n",
    "path_to_hdf5_file_val='D:/DEEPSLEEP/test_for_github/datasets/shhs/val/hdf5_file_val_all_chunking_shhs1.hdf5'\n",
    "utils.hdf5_creation1(mat_files[training_set:validation_set],path_to_hdf5_file_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Training and validation of the STQS model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_val.py\n",
    "or\n",
    "python train_val.py --batch_size=192 --n_workers=16 --learning_rate=0.0001 --max_epochs=200 --time_steps=3750 --n_channels=5 --modality_pipelines-3 --seq_length=8 --class_imbalance='Ã³versampling' --lstm_option=False --rc_option=False --patience_epoch=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible arguments:\n",
    "1. batch_size = size of the mini-batch passed for training the model; default=192\n",
    "2. n_workers = number of cpu cores to be used by data loader function of pytorch; default=16\n",
    "3. time_steps = 30 secs x sampling rate = 30x125=3750 (for SHHS dataset)\n",
    "4. max_epochs = Maximum number of epochs that a model will be trained if the training does not stop by early stopping; default=200\n",
    "5. n_channels = total number of channels in all modalities (EEG, EOG, EMG); for our model trained on SHHS dataset default=5 \n",
    "6. seq_length = sequence length for lstm; default=8 \n",
    "7. lstm_option = if False, the architecture only contains the CNN part (ST part); if True, the architecture contains the CNN+LSTM part, but not the residual connection; default=False\n",
    "8. rc_option = if False, the model does not contain the Residual connection block; if True, the model contains the residual connection block; default=False\n",
    "9. class_imbalance = Any value among ['None','oversampling','weightedcostfunc1','weightedcostfunc2']. 'None' corresponds to no class imbalance handling and the other values correspond to the way the class imbalance is handled.\n",
    "10. lr = learning rate; default=0.0001\n",
    "11. modality_pipeline = number of modality pipelines in the model; default=3 (EEG, EOG, EMG pipelines)\n",
    "12. patience_epoch = number of epochs to check before early stopping, i.e. stop training if validation loss does not decrease for n consecutive epochs; default=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Testing of STQS model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 eeg_annotation_shhs1-200011\n",
      "1 eeg_annotation_shhs1-200012\n",
      "0 eeg_annotation_shhs1-200011\n",
      "(989, 1, 5, 3750)\n",
      "(989, 1)\n",
      "1 eeg_annotation_shhs1-200012\n",
      "(964, 1, 5, 3750)\n",
      "(964, 1)\n",
      "dset data shape outside: (1953, 1, 5, 3750)\n",
      "dset label shape outside: (1953, 1)\n"
     ]
    }
   ],
   "source": [
    "mat_files=[]\n",
    "for i in os.listdir(path_to_mat_folder):\n",
    "    mat_files.append(i)\n",
    "\n",
    "sample_test_files=mat_files[10:12]\n",
    "\n",
    "path_to_file_length_test='D:/DEEPSLEEP/test_for_github/datasets/shhs/test/testFilesNum30secEpochs_all_shhs1.pkl'\n",
    "utils.calculate_num_samples(sample_test_files,path_to_file_length_test)\n",
    "\n",
    "path_to_hdf5_file_test='D:/DEEPSLEEP/test_for_github/datasets/shhs/test/hdf5_file_test_all_chunking_shhs1.hdf5'\n",
    "utils.hdf5_creation1(sample_test_files,path_to_hdf5_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
